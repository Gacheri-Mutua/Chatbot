{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d044024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict \n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import display, Image  \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d6c08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#loading the api keys\n",
    "load_dotenv() \n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if the key is loaded successfully\n",
    "if groq_api_key:\n",
    "    print(\"Groq API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"Error: Groq API Key not found. Check your .env file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c9c5e",
   "metadata": {},
   "source": [
    "TypeDict is a dictionary that allows for definition of the expected structure of dictionary objects including the type of each key-value pair while still using the familiar dictionary syntax.\n",
    "A class named State is defined using TypeDict and it has four attributes;\n",
    "query: str - holds a string that represents a customer query or question.\n",
    "category: str - store a string that categorizes the query. In this particular case, technical,billing and general.\n",
    "sentiment: str - captures the sentiment of the query\n",
    "response: str - hold the response generated for the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f508df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are transducers?\n",
      "Category: technical\n",
      "Sentiment: neutral\n",
      "Response: Transducers are devices that convert one form of energy into another.\n"
     ]
    }
   ],
   "source": [
    "#Creating the state class\n",
    "class State(TypedDict): \n",
    "    query: str\n",
    "    category: str\n",
    "    sentiment: str \n",
    "    response: str \n",
    "\n",
    "example_state: State = {\n",
    "    \"query\": \"What are transducers?\",\n",
    "    \"category\": \"technical\",\n",
    "    \"sentiment\": \"neutral\",\n",
    "    \"response\": \"Transducers are devices that convert one form of energy into another.\"\n",
    "}\n",
    "\n",
    "print(f\"Query: {example_state['query']}\")\n",
    "print(f\"Category: {example_state['category']}\")\n",
    "print(f\"Sentiment: {example_state['sentiment']}\")\n",
    "print(f\"Response: {example_state['response']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1840b0f",
   "metadata": {},
   "source": [
    "ChatGroq() creates a model using Groq\n",
    "temperature = 0, - the model will only generate straightforward & focused responses\n",
    "groq_api_key = groq_api_key - the password to use the model\n",
    "model_name = \"llama-3.3-70b-versatile\" - the specific llm is llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5be9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the llm\n",
    "llm = ChatGroq( \n",
    "    temperature = 0,\n",
    "    groq_api_key = groq_api_key,\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc06a3",
   "metadata": {},
   "source": [
    ".invoke() sends a prompt (the string \"what is a cat\") to the AI and gets a response\n",
    ".content extracts the text answer from the result object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca86967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital city of Australia is Canberra.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the model\n",
    "result = llm.invoke(\"what is the capital city of australia\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20bd85",
   "metadata": {},
   "source": [
    "**def categorize(state : State) -> State:**\n",
    "The function categorize takes an input parameter called state, State (the dictionary above), -> State makes sure the fucntion will return something of the same State type\n",
    "\n",
    "**prompt = ChatPromptTemplate.from_template** - \n",
    "Creates a template to for structuring queries into the three categories\n",
    "\n",
    "**chain = prompt | llm**- \n",
    "| (pipe) operator connects the prompt template and the language model and links the prompt to the llm for processing\n",
    "\n",
    "**category = chain.invoke({\"query\" : state['query']}).content** - \n",
    "Gets query from the state dicionary, sends it to the chain and triggers the model to process and respond to the query.\n",
    ".content gives back the text output\n",
    "\n",
    "**return {\"category\": category}**- this is a function that returs the dictionary \"category\" associated with the value of the variable category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8db2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#categorizing function\n",
    "def categorize(state : State) -> State:\n",
    "     \"Technical, Billing, General\"\n",
    "     prompt = ChatPromptTemplate.from_template(\n",
    "      \"Categorize the following customer query into one of these categories: \"\n",
    "      \"Technical, Billing, General. Query: {query}\"\n",
    "     )\n",
    "     chain = prompt | llm\n",
    "     category = chain.invoke({\"query\" : state['query']}).content\n",
    "     return {\"category\": category}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e2c7f",
   "metadata": {},
   "source": [
    "**def analyze_sentiment(state: State) -> State:**\n",
    "defines a function that takes a state object as input and returns a new state object\n",
    "\n",
    "**prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of the following customer query. \"\n",
    "    \"Respond with either 'Positive', 'Neutral', or 'Negative'. Query: {query}\"\n",
    ")** - This creates a prompt template and includes instructions to analyze the sentiment of a customer query and respond with one of three possible sentiments: 'Positive', 'Neutral', or 'Negative'. The {query} will be replaced with the actual query from the input state.\n",
    "\n",
    "**chain = prompt | llm**- \n",
    "| (pipe) operator connects the prompt template and the language model and links the prompt to the llm for processing\n",
    "\n",
    "**sentiment = chain.invoke({\"query\": state['query']}).content** - a chain is created then executed with the query from the state. The response is stored in the sentiment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8b7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing sentiment function\n",
    "def analyze_sentiment(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Analyze the sentiment of the following customer query. \"\n",
    "        \"Respond with either 'Positive', 'Neutral', or 'Negative'. Query: {query}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    sentiment = chain.invoke({\"query\": state['query']}).content  # Capture sentiment\n",
    "    return {\"sentiment\": sentiment}  # Return sentiment instead of category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a601f6",
   "metadata": {},
   "source": [
    "**def handle_technical(state : State) -> State:** - \n",
    "The function handle_technical takes a state object as input and returns a new state object.\n",
    "\n",
    "**prompt = ChatPromptTemplate.from_template(\"Provide a technical support response to the following query : {query}\")** - \n",
    "Instructs the model to provide support response to the query\n",
    "\n",
    "**chain = prompt | llm**- \n",
    "| (pipe) operator connects the prompt template and the language model and links the prompt to the llm for processing\n",
    "\n",
    "**response = chain.invoke({\"query\" : state['query']}).content return {\"response\" : response}**\n",
    "executes the chain from the input and returns an output response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fee7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#handle technical function\n",
    "def handle_technical(state : State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a technical support response to the following query : {query}\"\n",
    "        )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\" : state['query']}).content\n",
    "    return {\"response\" : response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeac5d7",
   "metadata": {},
   "source": [
    "Creates a prompt template for a general support response and pipes the prompt into the llm to create a chain and invokes it with the query from the state object. The response is captured from the chain invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1d87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling billing function\n",
    "def handle_billing(state : State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a billing support response to the following query : {query}\"\n",
    "\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\" : state['query']}).content\n",
    "    return {\"response\" : response}\n",
    "\n",
    "#handle general\n",
    "def handle_general(state : State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a general support response to the following query : {query}\"\n",
    "\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\" : state['query']}).content\n",
    "    return {\"response\" : response}\n",
    "\n",
    "#escalate the situation\n",
    "def escalate(state : State) -> State:\n",
    "    return{\"response\" : \"This query has been escalate to a human agent due to its negative sentiment\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00883890",
   "metadata": {},
   "source": [
    "If the sentiment of the query is negative, it returns the string \"escalate\".\n",
    "If the sentiment is not negative, it checks the category of the query.\n",
    "If the category is \"Technical\", it returns the string \"handle_technical\".\n",
    "If the category is \"Billing\", it returns the string \"handle_billing\".\n",
    "If the category doesn't match any of the above conditions, it returns the string \"handle_general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2754f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling the routing function\n",
    "def route_query(state: State) -> State:\n",
    "    if state['sentiment'] == 'Negative':\n",
    "        return \"escalate\"\n",
    "    elif state['category'] == 'Technical':\n",
    "        return \"handle_technical\"\n",
    "    elif state['category'] == 'Billing':\n",
    "        return \"handle_billing\"\n",
    "    else:\n",
    "        return \"handle_general\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b21e6d",
   "metadata": {},
   "source": [
    "### Query Handling Workflow\n",
    "\n",
    "* Start by **categorizing** the query.  \n",
    "* Check the **sentiment** of the query.  \n",
    "* Send the query to the right handler based on category and sentiment:  \n",
    "   - Technical → `handle_technical`  \n",
    "   - Billing → `handle_billing`  \n",
    "   - General → `handle_general`  \n",
    "   - Negative sentiment → `escalate`  \n",
    "* Each handler finishes the process.  \n",
    "* The workflow is compiled into `app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1638f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crafting the workflow\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"categorize\", categorize)\n",
    "workflow.add_node(\"analyze_sentiment\", analyze_sentiment)\n",
    "workflow.add_node(\"handle_technical\", handle_technical)\n",
    "workflow.add_node(\"handle_billing\", handle_billing)\n",
    "workflow.add_node(\"handle_general\", handle_general)\n",
    "workflow.add_node(\"escalate\", escalate)\n",
    "\n",
    "#adding edges\n",
    "workflow.add_edge(\"categorize\", \"analyze_sentiment\")\n",
    "workflow.add_conditional_edges(\"analyze_sentiment\", \n",
    "route_query,{\n",
    "    \"handle_technical\" : \"handle_technical\",\n",
    "    \"handle_billing\": \"handle_billing\",\n",
    "    \"handle_general\" : \"handle_general\",\n",
    "    \"escalate\": \"escalate\"\n",
    "})\n",
    "workflow.add_edge(\"handle_technical\", END)\n",
    "workflow.add_edge(\"handle_billing\", END)\n",
    "workflow.add_edge(\"handle_general\", END)\n",
    "workflow.add_edge(\"escalate\", END)\n",
    "\n",
    "#aadding the entry point\n",
    "workflow.set_entry_point(\"categorize\")\n",
    "\n",
    "#compile the workflow\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d2ea082",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#visualizing the workflow\u001b[39;00m\n\u001b[32m      2\u001b[39m display(\n\u001b[32m      3\u001b[39m     Image(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMermaidDrawMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPI\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m ))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\runnables\\graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:310\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    304\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    305\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    306\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    307\u001b[39m         )\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    459\u001b[39m     msg = (\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "#visualizing the workflow\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method = MermaidDrawMethod.API\n",
    "        )\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function to run customer support\n",
    "def run_customer_support(query: str) -> dict:\n",
    "    result = app.invoke({\"query\": query})  \n",
    "    return {\n",
    "        \"category\": result['category'], \n",
    "        \"sentiment\": result['sentiment'],\n",
    "        \"response\": result['response'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e70e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I need to bake a cake. Can you help me?\n",
      "Category: I would categorize the customer query as \"General\". The query is not related to a specific product or service issue (Technical) and does not involve payment or account information (Billing). It appears to be a general inquiry or request for assistance with a non-technical topic, in this case, baking a cake.\n",
      "Sentiment: Neutral. The customer is simply stating a need and asking for help, without expressing any emotion or opinion that would indicate a positive or negative sentiment.\n",
      "Response: Baking a cake can be a fun and rewarding experience. I'd be happy to help you with that. To get started, could you please provide me with a bit more information about the cake you want to bake? For example:\n",
      "\n",
      "* What type of cake are you looking to make (e.g. vanilla, chocolate, birthday cake, etc.)?\n",
      "* How many people are you planning to serve?\n",
      "* Do you have any specific ingredients or dietary restrictions in mind (e.g. gluten-free, vegan, etc.)?\n",
      "* Do you have a recipe in mind, or would you like me to suggest one?\n",
      "\n",
      "Once I have a better understanding of your needs, I can provide you with more tailored guidance and support to help you bake a delicious cake.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing the output\n",
    "query = \"I need to bake a cake. Can you help me?\"\n",
    "result = run_customer_support(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Category: {result['category']}\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64522157",
   "metadata": {},
   "source": [
    "#### Building a Simple UI for a Customer Support Assistant\n",
    "* Developing a  User Interface where you will enter your query, and the system will:  \n",
    "\n",
    "- Determine the **Category** (Technical, Billing, or General).  \n",
    "-  Analyze the **Sentiment** (Positive, Neutral, or Negative).  \n",
    "- Provide an appropriate **Response** or escalate if needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb77ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradio_interface(query: str):\n",
    "    result = run_customer_support(query)  \n",
    "    return (\n",
    "        f\"**Category:** {result['category']}\\n\\n\"\n",
    "        f\"**Sentiment:** {result['sentiment']}\\n\\n\"\n",
    "        f\"**Response:** {result['response']}\"\n",
    "    )\n",
    "\n",
    "# Building the Gradio app\n",
    "gui = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    theme='Yntec/HaleyCH_Theme_Orange_Green',\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your query here...\"),\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Customer Support Assistant\",\n",
    "    description=\"Ask a question and get a categorized, sentiment-aware response from the system.\"\n",
    ")\n",
    "\n",
    "# Launching the app\n",
    "if __name__ == \"__main__\":\n",
    "    gui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
